{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3455ac9c",
      "metadata": {
        "id": "3455ac9c"
      },
      "source": [
        "# Setting up Python environment - Installing dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive (to get access to the GitHub repository we cloned there)"
      ],
      "metadata": {
        "id": "oz6eC6Z9BQve"
      },
      "id": "oz6eC6Z9BQve"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8120199-85d4-4049-a9fd-192e4fd3a696",
      "metadata": {
        "id": "f8120199-85d4-4049-a9fd-192e4fd3a696"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/20240216_demo/pytorch_graph_demo\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "2XaA04qlBbUg"
      },
      "id": "2XaA04qlBbUg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install notebook\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install torch_geometric\n",
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
        "!pip install matplotlib\n",
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "mIC6O8IN8lAb"
      },
      "id": "mIC6O8IN8lAb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check install of dependencies"
      ],
      "metadata": {
        "id": "rwA6e8sZBdCd"
      },
      "id": "rwA6e8sZBdCd"
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "libraries = [\n",
        "    \"notebook\",\n",
        "    \"torch\",\n",
        "    \"torch_cluster\",\n",
        "    \"numpy\",\n",
        "    \"torch_geometric\",\n",
        "    \"matplotlib\",\n",
        "    \"rasterio\",\n",
        "    \"glob\"\n",
        "]\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        if not importlib.util.find_spec(lib):\n",
        "            raise ImportError(\"Package '%s' is not installed\" % lib)\n",
        "    except ImportError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "b1LBQINkAib5"
      },
      "id": "b1LBQINkAib5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports (first group installed packages/second group contains imports from files in cloned repository)"
      ],
      "metadata": {
        "id": "gN6XLuk1Birm"
      },
      "id": "gN6XLuk1Birm"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_geometric.transforms as T\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import classification\n",
        "import train\n",
        "import utils\n",
        "from Graph import AirQualityClassification, AirQualityRegression"
      ],
      "metadata": {
        "id": "HBotOK6VAkcA"
      },
      "id": "HBotOK6VAkcA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e75b3bc0",
      "metadata": {
        "id": "e75b3bc0"
      },
      "source": [
        "# PyG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf79e2d7",
      "metadata": {
        "id": "bf79e2d7"
      },
      "source": [
        "[PyG](https://pytorch-geometric.readthedocs.io/en/latest/index.html) (PyTorch Geometric) is a library built upon  [PyTorch](https://pytorch.org/) to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2746a991",
      "metadata": {
        "id": "2746a991"
      },
      "source": [
        "PyG provides two abstract classes for datasets: `torch_geometric.data.Dataset` and `torch_geometric.data.InMemoryDataset`.\n",
        "\n",
        "The [latter](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.InMemoryDataset.html#torch_geometric.data.InMemoryDataset) inherits from the former and is designed to be used when your dataset fits into CPU memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1f9e08",
      "metadata": {
        "id": "8d1f9e08"
      },
      "source": [
        "In this session we will use a class which has been written to inherit from `torch_geometric.data.InMemoryDataset` to load our remote sensing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18d4150",
      "metadata": {
        "id": "c18d4150"
      },
      "source": [
        "We use two classes (found in the `Graph.py` file in this repository)\n",
        "\n",
        "* `AirQualityClassification`\n",
        "* `AirQualityRegression`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3af9fca",
      "metadata": {
        "id": "d3af9fca"
      },
      "outputs": [],
      "source": [
        "aq_regression = AirQualityRegression(\n",
        "    seed=0, pre_transform=T.KNNGraph(k=10, force_undirected=True), train_ratio=0.2, val_ratio=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9acff5a",
      "metadata": {
        "id": "c9acff5a"
      },
      "outputs": [],
      "source": [
        "aq_classification = AirQualityClassification(\n",
        "    seed=0, pre_transform=T.KNNGraph(k=10, force_undirected=True), train_ratio=0.2, val_ratio=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be23e40",
      "metadata": {
        "id": "7be23e40"
      },
      "source": [
        "When we create an instance of our class (as performed above), because it is based on the `torch_geometric.data.InMemoryDataset` class, it performs two key operations:\n",
        "\n",
        "* If any of the files in `InMemoryDataset.raw_file_names()` are missing, then the `InMemoryDataset.download()` method is performed\n",
        "\n",
        "* If any of the files in `InMemoryDataset.processed_file_names()` are missing, then the `InMemoryDataset.process()` method is performed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ae2216",
      "metadata": {
        "id": "79ae2216"
      },
      "source": [
        "Because we have already have the remote sensing input files downloaded in this repository (inside `data` folder) you will notice that neither `AirQualityRegression` or `AirQualityClassification` provide a `download()` method or a `raw_file_names` property. Nothing happens for that part in our example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a8caec",
      "metadata": {
        "id": "62a8caec"
      },
      "outputs": [],
      "source": [
        "print(aq_regression.has_download)\n",
        "print(aq_classification.has_download)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "803d0544",
      "metadata": {
        "id": "803d0544"
      },
      "source": [
        "We do provide processing steps though. These load the inputs and define how we get our dataset graph's:\n",
        "\n",
        "* `x` input features\n",
        "* `y` output values/labels\n",
        "* `edge_index` (describes how graph nodes are connected i.e. could be used to find graph adjacency matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fbbb67f",
      "metadata": {
        "id": "6fbbb67f"
      },
      "source": [
        "> **To apply a similar approach to your own datasets you can borrow from the classes in `Graph.py` and write new/modified `process()` methods.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "507caacd",
      "metadata": {
        "id": "507caacd"
      },
      "source": [
        "The code inside the `process` function should write file(s) returned in `InMemoryDataset.processed_file_names()` list. The next time you create an instance of the class it will load from file rather than running `process()` again."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f84ca3",
      "metadata": {
        "id": "34f84ca3"
      },
      "source": [
        "# Loading our classification dataset: the `process()` method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f3f13c",
      "metadata": {
        "id": "48f3f13c"
      },
      "source": [
        "We use two functions written in the repository's `utils.py` file:\n",
        "\n",
        "* `utils.open_pm25()`\n",
        "* `utils.open_land_cover()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb5022c",
      "metadata": {
        "id": "feb5022c"
      },
      "source": [
        "### Inputs (air quality values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53fbd04f",
      "metadata": {
        "id": "53fbd04f"
      },
      "outputs": [],
      "source": [
        "pm25 = utils.open_pm25()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869b51c6",
      "metadata": {
        "id": "869b51c6"
      },
      "outputs": [],
      "source": [
        "print(pm25.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b20d17",
      "metadata": {
        "id": "28b20d17"
      },
      "source": [
        "In this case we are loading [CAMS data](https://atmosphere.copernicus.eu/cams-air-quality-data-quality-assured) (a forecast) for the surface level PM2.5 concentration.\n",
        "\n",
        "These files are found in the folder `data/PM2_5`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4425134",
      "metadata": {
        "id": "b4425134"
      },
      "source": [
        "For each `.tif` file in the folder we have a matrix of PM2.5 concentrations over a region in North Italy. There is a file for each hour of the day for 92 days.\n",
        "\n",
        "The data is quite coarse (each pixel represents 10x10km area) so each matrix is of shape 45x65 and we have 2208 temporal data points. The result is an array of shape 45x65x2208."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76aaf96b",
      "metadata": {
        "id": "76aaf96b"
      },
      "source": [
        "Time series visualisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6804ac86",
      "metadata": {
        "id": "6804ac86"
      },
      "outputs": [],
      "source": [
        "for series in pm25.reshape(-1, pm25.shape[-1])[:10]:\n",
        "    plt.plot(series, lw=.1, marker='x')\n",
        "plt.xlim(0, 10)\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"PM2.5 concentration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aae288e",
      "metadata": {
        "id": "2aae288e"
      },
      "source": [
        "Single time stamp (i.e. region of North Italy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3820fdaf",
      "metadata": {
        "id": "3820fdaf"
      },
      "outputs": [],
      "source": [
        "mappable = plt.matshow(pm25[..., -1])\n",
        "plt.colorbar(mappable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b903bc",
      "metadata": {
        "id": "38b903bc"
      },
      "source": [
        "### Outputs (land cover classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a0aefe",
      "metadata": {
        "id": "08a0aefe"
      },
      "outputs": [],
      "source": [
        "land_cover = utils.open_land_cover()\n",
        "print(land_cover.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9e5a66",
      "metadata": {
        "id": "db9e5a66"
      },
      "source": [
        "In this demonstration we are going to try and predict a land cover class label from our air quality time series. The labels in this case are provided by the Copernicus Land cover products ([CORINE](https://land.copernicus.eu/en/products/corine-land-cover)).\n",
        "\n",
        "The CORINE product is available at different levels i.e. how specific/broad each land cover class is. Here we are considering the Level 2 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2230308b",
      "metadata": {
        "id": "2230308b"
      },
      "outputs": [],
      "source": [
        "land_cover_labels = utils.open_land_cover_colormap()\n",
        "for key in land_cover_labels.keys():\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf54064",
      "metadata": {
        "id": "ddf54064"
      },
      "outputs": [],
      "source": [
        "print(f\"Predicting one of {len(land_cover_labels)} classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3263a2aa",
      "metadata": {
        "id": "3263a2aa"
      },
      "source": [
        "### Variable normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e9d10a",
      "metadata": {
        "id": "94e9d10a"
      },
      "source": [
        "In this example our input features (PM2.5 concentrations) vary from about 0 to 40.\n",
        "\n",
        "Generally speaking, it is good practice to perform normalisation of your data before you start optimising your model parameters. Without normalisation you might run into a number of issues with exploding gradients and generally poor convergence.\n",
        "\n",
        "There are many [excellent resources](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/) out there which discuss this topic in greater detail.\n",
        "\n",
        "If you look in `AirQualityRegression.process()` or `AirQualityClassification.process()` you will see I perform a min/max scaling to keep variables in the range 0 to 1. This method is sensitive to outliers so you may want to consider other more robust normalisation methods in your own research."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3875cc4",
      "metadata": {
        "id": "b3875cc4"
      },
      "source": [
        "## The graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60da839d",
      "metadata": {
        "id": "60da839d"
      },
      "source": [
        "Inside `AirQualityRegression.process()` and `AirQualityClassification.process()` we use `torch_geometric.data`'s `Data` class.\n",
        "\n",
        "This is the object which is eventually written to a file.\n",
        "\n",
        "In our application we provide three arguments:\n",
        "\n",
        "* `pos`\n",
        "* `x`\n",
        "* `y`\n",
        "\n",
        "We don't actually provide the graph connections here. It is possible, if you have edges you would explicitly like to use to pass them using the `edge_index` argument rather than `pos` here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a3002c",
      "metadata": {
        "id": "54a3002c"
      },
      "source": [
        "When we loaded the dataset you will notice we passed a `pre_transform` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9dd8f5d",
      "metadata": {
        "id": "c9dd8f5d"
      },
      "outputs": [],
      "source": [
        "aq_classification = AirQualityClassification(\n",
        "    seed=0, pre_transform=T.KNNGraph(k=10, force_undirected=True), train_ratio=0.2, val_ratio=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c340375",
      "metadata": {
        "id": "2c340375"
      },
      "source": [
        "`T.KNNGraph(k=10, force_undirected=True)` describes how the graph is connected. The [docs](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.KNNGraph.html#torch_geometric.transforms.KNNGraph) describe that it \"creates a k-NN graph based on node positions data.pos\"\n",
        "\n",
        "i.e. if you plotted each node's pos in space, you create a graph edge connecting each node to the k other nodes near it, in this case, the nearest 10 neighbours."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faeeb650",
      "metadata": {
        "id": "faeeb650"
      },
      "source": [
        "> Note, we use `force_undirected=True` to create a symmetric adjacency matrix meaning graph edges are not directional. If node A is connected to node B then vice versa is true also."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767968e3",
      "metadata": {
        "id": "767968e3"
      },
      "source": [
        "# Loading our regression dataset: the `process()` method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "646f30bb",
      "metadata": {
        "id": "646f30bb"
      },
      "source": [
        "### Inputs (air quality values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a71b62b",
      "metadata": {
        "id": "5a71b62b"
      },
      "outputs": [],
      "source": [
        "pm25 = utils.open_pm25()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3903b56c",
      "metadata": {
        "id": "3903b56c"
      },
      "outputs": [],
      "source": [
        "print(pm25.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "673fde93",
      "metadata": {
        "id": "673fde93"
      },
      "source": [
        "In this case we are loading [CAMS data](https://atmosphere.copernicus.eu/cams-air-quality-data-quality-assured) (a forecast) for the surface level PM2.5 concentration.\n",
        "\n",
        "These files are found in the folder `data/PM2_5`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd53dc9",
      "metadata": {
        "id": "1bd53dc9"
      },
      "source": [
        "For each `.tif` file in the folder we have a matrix of PM2.5 concentrations over a region in North Italy. There is a file for each hour of the day for 92 days.\n",
        "\n",
        "The data is quite coarse (each pixel represents 10x10km area) so each matrix is of shape 45x65 and we have 2208 temporal data points. The result is an array of shape 45x65x2208."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b2d8be0",
      "metadata": {
        "id": "8b2d8be0"
      },
      "source": [
        "Time series visualisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b08214",
      "metadata": {
        "id": "a1b08214"
      },
      "outputs": [],
      "source": [
        "for series in pm25.reshape(-1, pm25.shape[-1])[:10]:\n",
        "    plt.plot(series, lw=.1, marker='x')\n",
        "plt.xlim(0, 10)\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"PM2.5 concentration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5a0746",
      "metadata": {
        "id": "2c5a0746"
      },
      "source": [
        "Single time stamp (i.e. region of North Italy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27c2fd4",
      "metadata": {
        "id": "c27c2fd4"
      },
      "outputs": [],
      "source": [
        "mappable = plt.matshow(pm25[..., -1])\n",
        "plt.colorbar(mappable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abaaae93",
      "metadata": {
        "id": "abaaae93"
      },
      "source": [
        "### Outputs (DEM/height above sea level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09dd15fb",
      "metadata": {
        "id": "09dd15fb"
      },
      "outputs": [],
      "source": [
        "dem = utils.open_dem()\n",
        "print(land_cover.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fda944",
      "metadata": {
        "id": "19fda944"
      },
      "source": [
        "In this demonstration we have a toy example: to try and predict height above sea level from our air quality time series. The values in this case are provided by a downsampled [Copernicus DEM](https://spacedata.copernicus.eu/collections/copernicus-digital-elevation-model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36541c5",
      "metadata": {
        "id": "e36541c5"
      },
      "outputs": [],
      "source": [
        "mappable = plt.matshow(dem)\n",
        "plt.colorbar(mappable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8cdc0d",
      "metadata": {
        "id": "0c8cdc0d"
      },
      "source": [
        "### Variable normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d040deb8",
      "metadata": {
        "id": "d040deb8"
      },
      "source": [
        "In this example as well as normalising our input features (PM2.5 concentrations), we also normalise the outputs by performing min/max scaling to keep variables in the range 0 to 1.\n",
        "\n",
        "As mentioned before, min/max scaling is sensitive to outliers so you may want to consider other more robust normalisation methods.\n",
        "\n",
        "**We must remember to reverse this normalisation if we want to consider the model outputs in the original scale of meters above sea level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4324b8d4",
      "metadata": {
        "id": "4324b8d4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "62c8a12b",
      "metadata": {
        "id": "62c8a12b"
      },
      "source": [
        "# The neural network (GCN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699c72af",
      "metadata": {
        "id": "699c72af"
      },
      "source": [
        "In `train.py` you will find a class named `GCN`. It is an implementation based on a popular geometric neural network from [Kipf and Welling](https://arxiv.org/abs/1609.02907)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "426288d0",
      "metadata": {
        "id": "426288d0"
      },
      "source": [
        "## Graph convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d462dca",
      "metadata": {
        "id": "8d462dca"
      },
      "source": [
        "It has two graph convolutional layers, each of which use `torch_geometric.nn`'s `GCNConv` layer (with documentation [here](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html)).\n",
        "\n",
        "Mathematically it implements:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3f07dc",
      "metadata": {
        "id": "ca3f07dc"
      },
      "source": [
        "$$\\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "\\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ef0b73",
      "metadata": {
        "id": "36ef0b73"
      },
      "source": [
        "where $\\mathbf{X}^{\\prime}$ is the layer output, $\\mathbf{\\hat{D}}$ and $\\mathbf{\\hat{A}}$ are function's of the graph's adjacency matrix, $\\mathbf{X}$ is the input features and $\\mathbf{\\Theta}$ are the layer's parameters to be optimised."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13fa3f7",
      "metadata": {
        "id": "a13fa3f7"
      },
      "source": [
        "$\\mathbf{X}$ has $N$ rows and $C$ columns where $N$ is the number of nodes in the graph and $C$ is the number of features describing each node.\n",
        "\n",
        "$\\mathbf{\\Theta}$ has $C$ rows and $F$ features. You can choose $F$, it gives the number of output features per node in the layer's output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f330e2b9",
      "metadata": {
        "id": "f330e2b9"
      },
      "source": [
        "The operation $\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "\\mathbf{\\hat{D}}^{-1/2}$ is essentially a messsage passing step. The resulting output feature will be a weighted sum of the features of all the nodes in the imediate 1-hop vicinity of the given node.\n",
        "\n",
        "Think of it as a propagation/sharing/smoothing of features in the local area of the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5744761d",
      "metadata": {
        "id": "5744761d"
      },
      "source": [
        "In some applications where the graph doesn't change, it makes sense to compute $\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "\\mathbf{\\hat{D}}^{-1/2}$ only once and to store it:\n",
        "\n",
        "```\n",
        "        self._gcn0 = GCNConv(\n",
        "            ...\n",
        "            cached=True,\n",
        "            ...\n",
        "        )\n",
        "```\n",
        "\n",
        "This is what the `cached=True` argument does. If you need to compute $\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "\\mathbf{\\hat{D}}^{-1/2}$ each forward pass then set `cached=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a0ec20",
      "metadata": {
        "id": "56a0ec20"
      },
      "source": [
        "## Output activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51a42de1",
      "metadata": {
        "id": "51a42de1"
      },
      "source": [
        "$$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a09912",
      "metadata": {
        "id": "70a09912"
      },
      "source": [
        "For a classification task the softmax output activation function is generally chosen.\n",
        "Given a vector input it outputs a vector with entries that lie between 0 and 1 and when summed the entries equal 1. It can therefore be used to represent a probability distribution with each entry representing the probability that the model input belongs to the ith class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ea18f1",
      "metadata": {
        "id": "04ea18f1"
      },
      "source": [
        "For a regression task you can use a linear activation function (i.e. no activation function at all) since the output does not necessarily lie within a specific range (for most applications)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7674b66",
      "metadata": {
        "id": "d7674b66"
      },
      "source": [
        "## Model hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5143cf77",
      "metadata": {
        "id": "5143cf77"
      },
      "source": [
        "* number of channels, $C$ (i.e. how many features in the output of first GCNConv layer)\n",
        "* dropout_rate\n",
        "    * The GCN model has [dropout layers](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html). By default it is set to 0.5 but this can be varied from 0 to closer to 1.\n",
        "* activation function\n",
        "    * By default, the `relu` activation function is the non-linear activation function applied at the output of the first GCNConv layer. Other possible activation functions include Hyperbolic Tangent (Tanh), LeakyReLU, ...\n",
        "* use_bias `True` vs `False`\n",
        "    * If true then layer has additional parameters to learn i.e. an additive bias so that layer gives essentially a vector with $C$ entries which is added to each row of matrix given by $\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "\\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90244c53",
      "metadata": {
        "id": "90244c53"
      },
      "source": [
        "# Learning/optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78c7e17",
      "metadata": {
        "id": "f78c7e17"
      },
      "source": [
        "In `train.py` there are two training functions, one for the regression task, one for the classification task.\n",
        "\n",
        "Except for specific details they follow a similar pattern which we will go through now."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc77a8d1",
      "metadata": {
        "id": "dc77a8d1"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13c2175",
      "metadata": {
        "id": "e13c2175"
      },
      "source": [
        "```\n",
        "# Load data\n",
        "dataset = AirQualityClassification(\n",
        "    seed=seed, pre_transform=T.KNNGraph(k=10, force_undirected=True), train_ratio=0.2, val_ratio=0.1\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7568fd",
      "metadata": {
        "id": "6e7568fd"
      },
      "source": [
        "The `seed` argument determines the random splits (i.e. train/validation/test sets). If you change the seed you will split the datset differently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcf7d0a",
      "metadata": {
        "id": "bdcf7d0a"
      },
      "source": [
        "The `pre_transform=T.KNNGraph(k=10, force_undirected=True)` argument (as detailed earlier) defines that we want the a `KNNGraph` to be constructed connecting nearby nodes according to the dataset's `pos` property."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e90bad",
      "metadata": {
        "id": "b7e90bad"
      },
      "source": [
        "## Create an untrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2de4c7eb",
      "metadata": {
        "id": "2de4c7eb"
      },
      "source": [
        "```\n",
        "# Load an untrained model as described by the GCN class\n",
        "model = GCN(n_labels=n_classes, channels=32, output_activation=\"softmax\", use_bias=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cd594b",
      "metadata": {
        "id": "93cd594b"
      },
      "source": [
        "`n_labels` defines how many outputs we want (1 for regression or 1 per class for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d978f5",
      "metadata": {
        "id": "e0d978f5"
      },
      "source": [
        "`channels` is essentially $C$, the number of features in the output of first GCNConv layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "450c88c6",
      "metadata": {
        "id": "450c88c6"
      },
      "source": [
        "## Create an `optimizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91f6596",
      "metadata": {
        "id": "c91f6596"
      },
      "source": [
        "```\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=2.5e-4)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9757b68a",
      "metadata": {
        "id": "9757b68a"
      },
      "source": [
        "There are many [optimizer choices](https://pytorch.org/docs/stable/optim.html) but the `Adam` algorithm is usually safe first choice.\n",
        "\n",
        "The `learning_rate` essentially describes how large the update steps are during the optimisation. Too large and the optimisation may fail to converge. Too small and it will take a long time to converge.\n",
        "\n",
        "`weight_decay` is an L2 penalty (also described as regularization) which discourages model parameters with very large values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79a57d5",
      "metadata": {
        "id": "e79a57d5"
      },
      "source": [
        "## Choose a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a29b56a7",
      "metadata": {
        "id": "a29b56a7"
      },
      "source": [
        "```\n",
        "criterion = torch.nn.CrossEntropyLoss()  # cross entropy loss (for classification)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f54a0d",
      "metadata": {
        "id": "18f54a0d"
      },
      "source": [
        "It is the loss function which is minimised in order to train the model. It is important to choose a loss function which rewards models that provide desirable outputs.\n",
        "\n",
        "For a classification task, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) is a common choice. It penalises against predictions which assign low probability to a class which the training label says was true and vice versa.\n",
        "\n",
        "For a regression task, something like [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) or [L1Loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss) is a good place to start."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c98ee44",
      "metadata": {
        "id": "2c98ee44"
      },
      "source": [
        "## Optimisation loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d22d4ce6",
      "metadata": {
        "id": "d22d4ce6"
      },
      "source": [
        "```\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "    output = model(data)  # Perform a single forward pass.\n",
        "\n",
        "    loss = criterion(\n",
        "        output[dataset.mask_tr], data.y[dataset.mask_tr]\n",
        "    )  # Compute the loss solely based on the training nodes.\n",
        "\n",
        "    loss.backward()  # Derive gradients.\n",
        "\n",
        "    optimizer.step()  # Update parameters based on gradients (back-propagation).\n",
        "\n",
        "    with torch.no_grad():  # don't track gradients during validation\n",
        "        output = model(data)\n",
        "        val_loss = criterion(\n",
        "            output[dataset.mask_va], data.y[dataset.mask_va]\n",
        "        )  # Compute the loss solely based on the validation nodes.\n",
        "        if (\n",
        "            best_val_loss is None or val_loss <= best_val_loss\n",
        "        ):  # if validation loss is better than previous best, reset patience\n",
        "            torch.save(model, \"models/best_regression_model.pt\")  # checkpoint model\n",
        "            best_val_loss = val_loss\n",
        "            val_loss_patience = 0\n",
        "        else:\n",
        "            val_loss_patience += 1\n",
        "            if (\n",
        "                val_loss_patience >= patience\n",
        "            ):  # if validation loss hasn't improved in patience epochs, stop training\n",
        "                break\n",
        "\n",
        "    if (epoch + 1) % print_every == 0:  # every print_every epochs\n",
        "        print(f\"Epoch {epoch + 1}: loss: {loss:.3f} val_loss: {val_loss:.3f}\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a6bdf6",
      "metadata": {
        "id": "c0a6bdf6"
      },
      "source": [
        "The validation set is monitored during training to determine when to break out.\n",
        "\n",
        "Gradients are not computed on the validation set. When the validation loss stops improving or starts to grow, it indicates that we are beginning to overfit the training data.\n",
        "\n",
        "If left to continue overfitting, our model will likely generalise poorly. The test set predictions are not likely to be accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ef60d9",
      "metadata": {
        "id": "26ef60d9"
      },
      "source": [
        "## Training hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef90d87",
      "metadata": {
        "id": "5ef90d87"
      },
      "source": [
        "* choice of optimiser\n",
        "* `learning_rate`\n",
        "    * How big do we want our parameter step sizes to be?\n",
        "* `weight_decay`\n",
        "    * How heavily do we want penalise large magnitude model parameters? A form or regularization i.e. should help prevent overfitting\n",
        "* `patience`\n",
        "    * How many successive epochs do we allow the validation loss to not improve before breaking out from optimisation loop?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training regression model"
      ],
      "metadata": {
        "id": "mFQzqukLU8WW"
      },
      "id": "mFQzqukLU8WW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "xPZIJWriYRws"
      },
      "id": "xPZIJWriYRws"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_regression_model, regression_data, regression_masks = train.train_regression()"
      ],
      "metadata": {
        "id": "xqf0vL2-U9s2"
      },
      "id": "xqf0vL2-U9s2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions using trained model:"
      ],
      "metadata": {
        "id": "KghTTrJzYU54"
      },
      "id": "KghTTrJzYU54"
    },
    {
      "cell_type": "code",
      "source": [
        "mask_tr, mask_va, mask_te = regression_masks\n",
        "trained_regression_model.eval()  # double check that model is in evaluation mode\n",
        "with torch.no_grad():  # don't track gradients during validation\n",
        "    predictions = trained_regression_model(regression_data).cpu().numpy().flatten()"
      ],
      "metadata": {
        "id": "BDEXQbW-YeiB"
      },
      "id": "BDEXQbW-YeiB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results:"
      ],
      "metadata": {
        "id": "EyDBeTlyYLzr"
      },
      "id": "EyDBeTlyYLzr"
    },
    {
      "cell_type": "code",
      "source": [
        "# undo flattening\n",
        "ip = pm25.copy()\n",
        "gt_regression = dem.copy()\n",
        "mask = ~np.all(np.isnan(ip), axis=-1) * ~np.isnan(gt_regression)\n",
        "predictions_unflattened = np.zeros(ip.shape[:2]) * np.nan\n",
        "predictions_unflattened[mask] = predictions.flatten()\n",
        "\n",
        "# reverse normalization (i.e. DEM was normalized to [0, 1] during training, we want to reverse that)\n",
        "gt_unnormalized = gt_regression[mask]\n",
        "op_min, op_max = np.nanmin(gt_unnormalized), np.nanmax(gt_unnormalized)\n",
        "predictions_unflattened = predictions_unflattened * (op_max - op_min) + op_min\n",
        "\n",
        "vmin = min(np.nanmin(gt_regression), predictions_unflattened.min())\n",
        "vmax = max(np.nanmax(gt_regression), predictions_unflattened.max())\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, sharey=True, sharex=True, figsize=(12, 8))\n",
        "cbar = ax[0, 0].matshow(gt_regression, vmin=vmin, vmax=vmax)\n",
        "utils.add_colorbar(fig, cbar, ax[0, 0], x_shift=0.06)\n",
        "ax[0, 0].set_title(\"Ground truth DEM\")\n",
        "cbar = ax[0, 1].matshow(predictions_unflattened, vmin=vmin, vmax=vmax)\n",
        "utils.add_colorbar(fig, cbar, ax[0, 1], x_shift=0.06)\n",
        "ax[0, 1].set_title(\"Predicted DEM using PM2.5 time series\")\n",
        "\n",
        "gt_flattened = gt_regression[mask]\n",
        "gt_flattened[~mask_te] = np.nan  # show only test pixels\n",
        "gt_unflattened = np.zeros(ip.shape[:2]) * np.nan\n",
        "gt_unflattened[mask] = gt_flattened.flatten()\n",
        "cbar = ax[1, 0].matshow(gt_unflattened, vmin=vmin, vmax=vmax)\n",
        "utils.add_colorbar(fig, cbar, ax[1, 0], x_shift=0.06)\n",
        "ax[1, 0].set_title(\"Ground truth DEM (test nodes only)\")\n",
        "predictions[~mask_te] = np.nan  # show only test pixels\n",
        "predictions_unflattened = np.zeros(ip.shape[:2]) * np.nan\n",
        "predictions_unflattened[mask] = predictions.flatten()\n",
        "predictions_unflattened = predictions_unflattened * (op_max - op_min) + op_min\n",
        "cbar = ax[1, 1].matshow(predictions_unflattened, vmin=vmin, vmax=vmax)\n",
        "utils.add_colorbar(fig, cbar, ax[1, 1], x_shift=0.06)\n",
        "ax[1, 1].set_title(\"Predicted DEM (test nodes only)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2LIO4B1GYKmu"
      },
      "id": "2LIO4B1GYKmu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training classification model"
      ],
      "metadata": {
        "id": "sm6R400tY_KJ"
      },
      "id": "sm6R400tY_KJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "L4u6wYTEZEDh"
      },
      "id": "L4u6wYTEZEDh"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_classification_model, classification_data, classification_masks = train.train_classification()"
      ],
      "metadata": {
        "id": "PYHKDhXeZHty"
      },
      "id": "PYHKDhXeZHty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions using trained model:"
      ],
      "metadata": {
        "id": "MUlx7iFJZIwC"
      },
      "id": "MUlx7iFJZIwC"
    },
    {
      "cell_type": "code",
      "source": [
        "# undo flattening\n",
        "mask_tr, mask_va, mask_te = classification_masks\n",
        "\n",
        "trained_classification_model.eval()  # double check that model is in evaluation mode\n",
        "with torch.no_grad():  # don't track gradients during validation\n",
        "    predictions = trained_classification_model(classification_data).cpu().numpy()"
      ],
      "metadata": {
        "id": "KR5EHPUbZQBJ"
      },
      "id": "KR5EHPUbZQBJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot results:"
      ],
      "metadata": {
        "id": "x2AjPltwZRdR"
      },
      "id": "x2AjPltwZRdR"
    },
    {
      "cell_type": "code",
      "source": [
        "gt_classification = utils.open_land_cover() - 1\n",
        "mask = ~np.all(np.isnan(pm25), axis=-1) * ~np.isnan(gt_classification)\n",
        "unflattened_predictions = classification.unflatten(mask, np.argmax(predictions, axis=1))\n",
        "\n",
        "unflattened_mask_tr = classification.unflatten(mask, mask_tr, outside_value=False, dtype=bool)\n",
        "unflattened_mask_va = classification.unflatten(mask, mask_va, outside_value=False, dtype=bool)\n",
        "unflattened_mask_te = classification.unflatten(mask, mask_te, outside_value=False, dtype=bool)\n",
        "\n",
        "training_labels = gt_classification.copy()\n",
        "training_labels[~unflattened_mask_tr] = np.nan\n",
        "validation_labels = gt_classification.copy()\n",
        "validation_labels[~unflattened_mask_va] = np.nan\n",
        "test_labels = gt_classification.copy()\n",
        "test_labels[~unflattened_mask_te] = np.nan\n",
        "training_predictions = unflattened_predictions.copy()\n",
        "training_predictions[~unflattened_mask_tr] = np.nan\n",
        "validation_predictions = unflattened_predictions.copy()\n",
        "validation_predictions[~unflattened_mask_va] = np.nan\n",
        "test_predictions = unflattened_predictions.copy()\n",
        "test_predictions[~unflattened_mask_te] = np.nan\n",
        "\n",
        "class_colormap = utils.open_land_cover_colormap()\n",
        "mappable = ListedColormap(class_colormap.values())\n",
        "fig, ax = plt.subplots(3, 3, sharex=True, sharey=True, figsize=(15, 10))\n",
        "ax[0, 0].matshow(training_labels, cmap=mappable, vmin=-0.5, vmax=len(class_colormap) - 0.5)\n",
        "ax[0, 0].set_title(\"Ground truth (training)\")\n",
        "ax[0, 1].matshow(validation_labels, cmap=mappable, vmin=-0.5, vmax=len(class_colormap) - 0.5)\n",
        "ax[0, 1].set_title(\"Ground truth (val.)\")\n",
        "img = ax[0, 2].matshow(test_labels, cmap=mappable, vmin=-0.5, vmax=len(class_colormap) - 0.5)\n",
        "ax[0, 2].set_title(\"Ground truth (test)\")\n",
        "classification.add_colorbar(fig, img, class_colormap.keys(), ax[0, 2], x_shift=0.04)\n",
        "\n",
        "ax[1, 0].matshow(training_predictions, cmap=mappable, vmin=-0.5, vmax=len(class_colormap) - 0.5)\n",
        "ax[1, 0].set_title(\"Predictions (training)\")\n",
        "ax[1, 1].matshow(validation_predictions, cmap=mappable, vmin=-0.5, vmax=len(class_colormap) - 0.5)\n",
        "ax[1, 1].set_title(\"Predictions (val.)\")\n",
        "img = ax[1, 2].matshow(test_predictions, cmap=mappable, vmin=-0.5, vmax=len(class_colormap) - 0.5)\n",
        "ax[1, 2].set_xticks([])\n",
        "ax[1, 2].set_title(\"Predictions (test)\")\n",
        "classification.add_colorbar(fig, img, class_colormap.keys(), ax[1, 2], x_shift=0.04)\n",
        "\n",
        "mappable = ListedColormap([[1.0, 0, 0], [0, 1.0, 0]])\n",
        "ax[2, 0].matshow(\n",
        "    np.where(mask * unflattened_mask_tr, training_labels == training_predictions, np.nan), cmap=mappable, vmin=-0.5, vmax=1.5\n",
        ")\n",
        "ax[2, 0].set_title(\"Errors (training)\")\n",
        "ax[2, 1].matshow(\n",
        "    np.where(mask * unflattened_mask_va, validation_labels == validation_predictions, np.nan),\n",
        "    cmap=mappable,\n",
        "    vmin=-0.5,\n",
        "    vmax=1.5,\n",
        ")\n",
        "ax[2, 1].set_title(\"Errors (val.)\")\n",
        "img = ax[2, 2].matshow(\n",
        "    np.where(mask * unflattened_mask_te, test_labels == test_predictions, np.nan), cmap=mappable, vmin=-0.5, vmax=1.5\n",
        ")\n",
        "ax[2, 2].set_title(\"Errors (test)\")\n",
        "classification.add_colorbar(fig, img, [\"Differing labels\", \"Matching labels\"], ax[2, 2], x_shift=0.04)\n",
        "utils.axes_off(ax)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bjka7cCKZVEd"
      },
      "id": "Bjka7cCKZVEd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}